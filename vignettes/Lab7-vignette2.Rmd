---
title: "Lab7-Vignette2"
author: "Oscar Petterson and Vuong Tran"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Lab7-Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---


## 1

```{r}
library(caret)
library(mlbench)
library(Lab7)
data("BostonHousing")
```

We have data about 506 properties in Boston from 1970. 14 variables are included. We want to predict the crime rate (per capita), using the other 13 variables.

```{r}
head(BostonHousing)
```

We divide the dataset into 70 % training and 30 % test data.

```{r}
set.seed(1337)
inTraining <- caret::createDataPartition(BostonHousing$crim, p = .7, list = FALSE)
training <- BostonHousing[ inTraining,]
testing  <- BostonHousing[-inTraining,]
fitControl <- caret::trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 10)
```

## 2
We use the caret package and its train function to fit a linear regression model on the training set:

```{r}
lmFit <- caret::train(crim ~ ., data = training,
                      method = "lm",
                      trControl = fitControl
)
lmFit


```

Now, we try finding a linear regression model using forward selection, on the same training data as before:
  
```{r}
lmGrid <-  expand.grid(nvmax=1:(ncol(training)-1))
fsFit <- caret::train(crim ~ ., data = training,
                      method = "leapForward",
                      trControl = fitControl,
                      tuneGrid=lmGrid
)

fsFit
```

## 3
When comparing our two different methods lm and leapFoward, the result show that using only seven predictor is better than using all 13 of them with respect to the RMSE value. We can't compare the Rsquared values since lm and leapFowards methods give us two different best models with different amount of predictors.

## 4-5

We try the caret train function with our function ridgereg as methods with lambda, variating from 0 to 100 and see that the best values are when lambda is around 14-18. We made a new run, with focus on these values and noticed that the best values was when lambda was equal to 16.36.
The differences can be seen down here:
  
```{r}

lmRig<-list(type=c("Regression","Classification"),
            library="Lab7",
            loop=NULL)

lmRig$parameters<-data.frame(parameter="lambda",
                             class="numeric",
                             label="Lambda")

# 
# Fit<-function(formula,data,lambda,param,lev,last,classProbs,...){
# 
#   
#   Lab7::ridgereg( formula=formula,data=data,lambda=param$lambda)
# }


Fit<-function(x,y,lambda,param,lev,last,classProbs,...){
  
  dat <- as.data.frame(x)
  dat$response <- y
  
  formula <- "response ~ "
  
  formula <- paste(formula, names(dat)[1], sep="")
  
  if(ncol(x) > 1){
    for(i in 2:ncol(x)){
      
      formula <- paste(formula, " + ", names(dat)[i], sep="")
      
    }
  }
  
  formula <- as.formula(formula)
  Lab7::ridgereg( formula = formula, data=dat,lambda=param$lambda)
}

lmRig$fit<-Fit

lmRig$predict<-function (modelFit, newdata,preProc=NULL, submodels = NULL) 
{
  predict(modelFit, newdata)
}

lmRig$prob<- list(NULL)

lmRig$sort<-function (x) x[order(-x$lambda), ]

lmRig$label<-"Ridgeregression"

lmRig$grid<-function(formula,data,len=NULL){
  # data.frame(lambda=seq(16,16.5,0.01))
  # data.frame(lambda=seq(10,20,2))
  data.frame(lambda=c(0, 1, 5, 16.36, 20, 50, 100))
}


BostonHousing$chas <- as.numeric(BostonHousing$chas)-1



rrFit1 <- caret::train(crim ~ ., data = training,
                       method = lmRig,
                       trControl = fitControl
)
rrFit1
```

## 6
When our tuning parameter lambda is zero, ridge regression is equal to OLS and as we can see, the Root Mean Squared Error is much lower for the best tuning value.
We can't compare the RMSE between the ridge regression and the forward selection methods. Since regular lm and leapForward methods produced almost the same RMSE value our conclusion is that our Rigdgereg with best tuning paramether (lambda = 16.36) is also better than the best model from the leapForward method (linear regression with seven explanatory variables).

